# Inference serving configuration

model:
  # Path to merged + quantized model (GGUF) or adapter directory
  # After training, run `make export-gguf` to create the GGUF file
  gguf_path: "models/merged/seshat-qwen3-14b-Q4_K_M.gguf"
  # OR use adapter directly with base model:
  # base_model: "Qwen/Qwen3-14B"
  # adapter_path: "models/adapters/qwen3-14b-seshat"

llama_cpp:
  n_ctx: 4096                # Context window
  n_gpu_layers: -1           # -1 = offload all layers to GPU (if available)
  n_threads: 8               # CPU threads for non-GPU layers
  verbose: false

generation:
  temperature: 0.3           # Low temp for translation (more deterministic)
  top_p: 0.9
  top_k: 40
  max_tokens: 512
  repeat_penalty: 1.1

server:
  host: "0.0.0.0"
  port: 8000
